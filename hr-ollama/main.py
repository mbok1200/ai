import os
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import subprocess
import tempfile

def download_ollama_model_weights(model_name="gemma2:2b"):
    """
    –í–∏—Ç—è–≥—É—î –≤–∞–≥–∏ –º–æ–¥–µ–ª—ñ –∑ Ollama –≤ HuggingFace —Ñ–æ—Ä–º–∞—Ç
    """
    print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –º–æ–¥–µ–ª—å {model_name} –∑ Ollama...")
    
    # –ú–∞–ø—ñ–Ω–≥ Ollama –º–æ–¥–µ–ª–µ–π –Ω–∞ HuggingFace
    model_mapping = {
        "gemma2:2b": "google/gemma-2-2b",
        "gemma2:9b": "google/gemma-2-9b", 
        "llama3.1:8b": "meta-llama/Meta-Llama-3.1-8B-Instruct",
        "qwen2:7b": "Qwen/Qwen2-7B-Instruct"
    }
    
    hf_model_name = model_mapping.get(model_name)
    if not hf_model_name:
        print(f"‚ùå –ù–µ–≤—ñ–¥–æ–º–∞ –º–æ–¥–µ–ª—å: {model_name}")
        return None
    
    print(f"üîó –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ HuggingFace –º–æ–¥–µ–ª—å: {hf_model_name}")
    return hf_model_name

def merge_lora_with_ollama_model(ollama_model_name, lora_path, output_path):
    """
    –û–±'—î–¥–Ω—É—î LoRA –∞–¥–∞–ø—Ç–µ—Ä –∑ –º–æ–¥–µ–ª–ª—é –∑ Ollama
    """
    print(f"üöÄ –ü–æ—á–∞—Ç–æ–∫ –æ–±'—î–¥–Ω–∞–Ω–Ω—è {ollama_model_name} –∑ LoRA...")
    
    # –û—Ç—Ä–∏–º—É—î–º–æ HuggingFace –Ω–∞–∑–≤—É –º–æ–¥–µ–ª—ñ
    hf_model_name = download_ollama_model_weights(ollama_model_name)
    if not hf_model_name:
        return False
    
    try:
        print(f"üì• –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å {hf_model_name}...")
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ –±–∞–∑–æ–≤—É –º–æ–¥–µ–ª—å
        base_model = AutoModelForCausalLM.from_pretrained(
            hf_model_name,
            torch_dtype=torch.float16,
            device_map="auto",
            trust_remote_code=True,
            low_cpu_mem_usage=True
        )
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ —Ç–æ–∫–µ–Ω—ñ–∑–∞—Ç–æ—Ä
        tokenizer = AutoTokenizer.from_pretrained(hf_model_name, trust_remote_code=True)
        
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
        
        print(f"üîó –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ LoRA –∞–¥–∞–ø—Ç–µ—Ä –∑ {lora_path}...")
        
        # –ü–µ—Ä–µ–≤—ñ—Ä—è—î–º–æ —á–∏ —ñ—Å–Ω—É—î LoRA
        if not os.path.exists(os.path.join(lora_path, "adapter_config.json")):
            print(f"‚ùå LoRA –∞–¥–∞–ø—Ç–µ—Ä –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–æ –≤ {lora_path}")
            return False
        
        # –ó–∞–≤–∞–Ω—Ç–∞–∂—É—î–º–æ PEFT –º–æ–¥–µ–ª—å
        peft_model = PeftModel.from_pretrained(base_model, lora_path)
        
        print("üîÄ –û–±'—î–¥–Ω—É—î–º–æ LoRA –∑ –±–∞–∑–æ–≤–æ—é –º–æ–¥–µ–ª–ª—é...")
        # –û–±'—î–¥–Ω—É—î–º–æ –∞–¥–∞–ø—Ç–µ—Ä –∑ –±–∞–∑–æ–≤–æ—é –º–æ–¥–µ–ª–ª—é
        merged_model = peft_model.merge_and_unload()
        
        print(f"üíæ –ó–±–µ—Ä—ñ–≥–∞—î–º–æ –æ–±'—î–¥–Ω–∞–Ω—É –º–æ–¥–µ–ª—å –≤ {output_path}...")
        os.makedirs(output_path, exist_ok=True)
        
        # –ó–±–µ—Ä—ñ–≥–∞—î–º–æ —è–∫ –æ–¥–∏–Ω —Ñ–∞–π–ª (–Ω–µ sharded)
        merged_model.save_pretrained(
            output_path,
            max_shard_size="10GB",  # –û–¥–∏–Ω —Ñ–∞–π–ª
            safe_serialization=True
        )
        tokenizer.save_pretrained(output_path)
        
        print(f"‚úÖ –ú–æ–¥–µ–ª—å —É—Å–ø—ñ—à–Ω–æ –∑–±–µ—Ä–µ–∂–µ–Ω–∞ –≤: {output_path}")
        
        # –ü–æ–∫–∞–∑—É—î–º–æ —â–æ —Å—Ç–≤–æ—Ä–∏–ª–æ—Å—è
        files = os.listdir(output_path)
        print(f"üìÅ –°—Ç–≤–æ—Ä–µ–Ω—ñ —Ñ–∞–π–ª–∏: {files}")
        
        return True
        
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ–±'—î–¥–Ω–∞–Ω–Ω—ñ: {e}")
        import traceback
        traceback.print_exc()
        return False

def create_ollama_modelfile(model_path, model_name):
    """
    –°—Ç–≤–æ—Ä—é—î Modelfile –¥–ª—è Ollama
    """
    print(f"üìù –°—Ç–≤–æ—Ä—é—î–º–æ Modelfile –¥–ª—è {model_name}...")
    
    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∞–±—Å–æ–ª—é—Ç–Ω–∏–π —à–ª—è—Ö
    abs_model_path = os.path.abspath(model_path)
    
    modelfile_content = f"""FROM {abs_model_path}

TEMPLATE \"\"\"{{{{ if .System }}}}<start_of_turn>system
{{{{ .System }}}}<end_of_turn>
{{{{ end }}}}{{{{ if .Prompt }}}}<start_of_turn>user
{{{{ .Prompt }}}}<end_of_turn>
<start_of_turn>model
{{{{ end }}}}{{{{ .Response }}}}<end_of_turn>\"\"\"

PARAMETER stop "<start_of_turn>"
PARAMETER stop "<end_of_turn>"
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER num_ctx 4096

SYSTEM \"\"\"–¢–∏ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–∏–π HR –∞—Å–∏—Å—Ç–µ–Ω—Ç –∑ –≤–µ–ª–∏–∫–∏–º –¥–æ—Å–≤—ñ–¥–æ–º –≤ —É–ø—Ä–∞–≤–ª—ñ–Ω–Ω—ñ –ø–µ—Ä—Å–æ–Ω–∞–ª–æ–º.

–¢–≤–æ—ó –∑–∞–≤–¥–∞–Ω–Ω—è –Ω–∞ —Å—å–æ–≥–æ–¥–Ω—ñ –º–æ–∂—É—Ç—å –≤–∫–ª—é—á–∞—Ç–∏:
- –ü–µ—Ä–µ–≥–ª—è–¥ —Ç–∞ –æ—Ü—ñ–Ω–∫–∞ —Ä–µ–∑—é–º–µ –∫–∞–Ω–¥–∏–¥–∞—Ç—ñ–≤
- –ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–æ —Å–ø—ñ–≤–±–µ—Å—ñ–¥ —Ç–∞ —ó—Ö –ø—Ä–æ–≤–µ–¥–µ–Ω–Ω—è
- –ê–Ω–∞–ª—ñ–∑ –ø—Ä–æ–¥—É–∫—Ç–∏–≤–Ω–æ—Å—Ç—ñ –∫–æ–º–∞–Ω–¥–∏
- –†–æ–±–æ—Ç–∞ –∑ HR –º–µ—Ç—Ä–∏–∫–∞–º–∏ —Ç–∞ –∑–≤—ñ—Ç–∞–º–∏
- –ü–ª–∞–Ω—É–≤–∞–Ω–Ω—è –∑–∞—Ö–æ–¥—ñ–≤ –∑ —Ä–æ–∑–≤–∏—Ç–∫—É –ø–µ—Ä—Å–æ–Ω–∞–ª—É
- –í–∏—Ä—ñ—à–µ–Ω–Ω—è –∫–æ–Ω—Ñ–ª—ñ–∫—Ç–Ω–∏—Ö —Å–∏—Ç—É–∞—Ü—ñ–π
- –û–Ω–æ–≤–ª–µ–Ω–Ω—è HR –ø–æ–ª—ñ—Ç–∏–∫ —Ç–∞ –ø—Ä–æ—Ü–µ–¥—É—Ä

–í—ñ–¥–ø–æ–≤—ñ–¥–∞–π —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é –º–æ–≤–æ—é, —Å—Ç—Ä—É–∫—Ç—É—Ä–æ–≤–∞–Ω–æ —Ç–∞ –ø—Ä–æ—Ñ–µ—Å—ñ–π–Ω–æ. –ó–∞–≤–∂–¥–∏ –ø—Ä–æ–ø–æ–Ω—É–π –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ñ –¥—ñ—ó —Ç–∞ –ø—Ä–∞–∫—Ç–∏—á–Ω—ñ –ø–æ—Ä–∞–¥–∏.\"\"\"
"""
    
    modelfile_path = f"Modelfile.{model_name}"
    with open(modelfile_path, 'w', encoding='utf-8') as f:
        f.write(modelfile_content)
    
    print(f"‚úÖ Modelfile —Å—Ç–≤–æ—Ä–µ–Ω–æ: {modelfile_path}")
    return modelfile_path

def create_ollama_model(modelfile_path, model_name):
    """
    –°—Ç–≤–æ—Ä—é—î –º–æ–¥–µ–ª—å –≤ Ollama
    """
    print(f"üì§ –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å {model_name} –≤ Ollama...")
    
    try:
        cmd = ["ollama", "create", model_name, "-f", modelfile_path]
        print(f"üîÑ –í–∏–∫–æ–Ω—É—î–º–æ: {' '.join(cmd)}")
        
        result = subprocess.run(cmd, capture_output=True, text=True)
        
        if result.returncode == 0:
            print(f"‚úÖ –ú–æ–¥–µ–ª—å {model_name} —Å—Ç–≤–æ—Ä–µ–Ω–∞ —É—Å–ø—ñ—à–Ω–æ!")
            print(f"üöÄ –î–ª—è –∑–∞–ø—É—Å–∫—É: ollama run {model_name}")
            return True
        else:
            print(f"‚ùå –ü–æ–º–∏–ª–∫–∞ —Å—Ç–≤–æ—Ä–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ:")
            print(f"stdout: {result.stdout}")
            print(f"stderr: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"‚ùå –ü–æ–º–∏–ª–∫–∞: {e}")
        return False

def main():
    """
    –ì–æ–ª–æ–≤–Ω–∞ —Ñ—É–Ω–∫—Ü—ñ—è
    """
    print("ü§ñ HR Assistant LoRA + Ollama Model Merger")
    print("=" * 50)
    
    # –ù–∞–ª–∞—à—Ç—É–≤–∞–Ω–Ω—è
    ollama_model = "gemma2:2b"                    # –ú–æ–¥–µ–ª—å –∑ Ollama
    lora_path = "./lora_adapter"                  # –í–∞—à LoRA –∞–¥–∞–ø—Ç–µ—Ä
    merged_model_path = "./merged_hr_model"       # –ü–∞–ø–∫–∞ –¥–ª—è –æ–±'—î–¥–Ω–∞–Ω–æ—ó –º–æ–¥–µ–ª—ñ
    final_model_name = "hr_assistant_local"      # –ù–∞–∑–≤–∞ –≤ Ollama
    
    try:
        # 1. –û–±'—î–¥–Ω—É—î–º–æ LoRA –∑ –±–∞–∑–æ–≤–æ—é –º–æ–¥–µ–ª–ª—é
        print(f"üîÑ –ö—Ä–æ–∫ 1: –û–±'—î–¥–Ω–∞–Ω–Ω—è {ollama_model} –∑ LoRA...")
        success = merge_lora_with_ollama_model(ollama_model, lora_path, merged_model_path)
        
        if not success:
            print("‚ùå –ù–µ –≤–¥–∞–ª–æ—Å—è –æ–±'—î–¥–Ω–∞—Ç–∏ –º–æ–¥–µ–ª—å –∑ LoRA")
            return
        
        # 2. –°—Ç–≤–æ—Ä—é—î–º–æ Modelfile
        print(f"üîÑ –ö—Ä–æ–∫ 2: –°—Ç–≤–æ—Ä–µ–Ω–Ω—è Modelfile...")
        modelfile_path = create_ollama_modelfile(merged_model_path, final_model_name)
        
        # 3. –°—Ç–≤–æ—Ä—é—î–º–æ –º–æ–¥–µ–ª—å –≤ Ollama
        print(f"üîÑ –ö—Ä–æ–∫ 3: –Ü–º–ø–æ—Ä—Ç –≤ Ollama...")
        success = create_ollama_model(modelfile_path, final_model_name)
        
        if success:
            print(f"\nüéâ –£—Å–ø—ñ—Ö! –í–∞—à HR Assistant –≥–æ—Ç–æ–≤–∏–π!")
            print(f"üìã –ö–æ–º–∞–Ω–¥–∏ –¥–ª—è –≤–∏–∫–æ—Ä–∏—Å—Ç–∞–Ω–Ω—è:")
            print(f"   ollama run {final_model_name}")
            print(f"   >>> —è–∫—ñ –≤ –º–µ–Ω–µ –∑–∞–≤–¥–∞–Ω–Ω—è –Ω–∞ —Å—å–æ–≥–æ–¥–Ω—ñ")
            
            # –¢–µ—Å—Ç—É—î–º–æ –º–æ–¥–µ–ª—å
            print(f"\nüß™ –•–æ—á–µ—Ç–µ –ø—Ä–æ—Ç–µ—Å—Ç—É–≤–∞—Ç–∏ –º–æ–¥–µ–ª—å –∑–∞—Ä–∞–∑? (y/n)")
            if input().lower() == 'y':
                os.system(f"ollama run {final_model_name}")
        
    except Exception as e:
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–Ω–∞ –ø–æ–º–∏–ª–∫–∞: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()