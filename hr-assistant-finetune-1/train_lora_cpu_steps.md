# Кроки тренування LoRA на GPT-Neo 1.3B

## 1. Вибір моделі і пристрою
- Вказується ім'я моделі (`EleutherAI/gpt-neo-1.3B`).
- Визначається пристрій для тренування: GPU (cuda) або CPU.

## 2. Токенайзер
- Завантажується токенайзер для обраної моделі.
- Встановлюється токен для паддінгу як кінець послідовності (`eos_token`).

## 3. Модель
- Завантажується модель для задачі Causal Language Modeling.
- Вмикається gradient checkpointing для економії пам'яті.
- Вимикається кешування для сумісності з LoRA.

## 4. LoRA конфіг
- Створюється конфігурація LoRA (`LoraConfig`).
- Вказуються параметри: розмір рангу, dropout, цільові модулі, тощо.
- Модель обгортається у LoRA через `get_peft_model`.

## 5. Датасет
- Завантажується датасет з файлу у форматі JSONL.
- Форматується кожен приклад у вигляді інструкції, входу та виходу.
- Додається посилання на джерело, якщо воно є.
- Токенізується текст, обрізається до 512 токенів, падиться до фіксованої довжини.

## 6. Параметри тренування
- Встановлюються параметри тренування (`TrainingArguments`):
  - Каталог для збереження чекпойнтів.
  - Розмір батчу, кількість епох, кроки логування/збереження.
  - Тип оптимізатора, використання fp16/bf16 для GPU.

## 7. Тренер
- Створюється об'єкт `Trainer` з моделлю, аргументами, датасетом та коллатором.

## 8. Запуск тренування
- Запускається тренування моделі методом `train()`.

### Приклад логу тренування:
```
{'loss': 1.2231, 'grad_norm': 0.46188926696777344, 'learning_rate': 2.027363184079602e-05, 'epoch': 0.6}
```

## 9. Збереження
- Зберігається адаптер LoRA та токенайзер у вказану директорію.
