# -*- coding: utf-8 -*-
"""LoRA_Training_Colab.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JaN9ytPJCvwwnMEa8qbCepoheMldzCeT

# LoRA Model Training in Google Colab

This notebook is designed to train a LoRA model using the provided dataset and the training script from `train_lora.py`. Follow the steps below to set up the environment and execute the training process.
"""

!pip install -q peft transformers datasets accelerate

# Load the dataset
import json

with open('merged_min.jsonl', 'r', encoding='utf-8') as f:
    data = [json.loads(line) for line in f]

print(f"Loaded {data} examples.")
from datasets import Dataset

dataset = Dataset.from_list(data)
print(f'Dataset loaded with {len(dataset)} examples.')

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling

import sys
sys.modules["bitsandbytes"] = None
from peft import get_peft_model, LoraConfig, TaskType
class NoToTrainer(Trainer):
    def _move_model_to_device(self, model, device):
        # –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ –ø–µ—Ä–µ–º—ñ—â–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ
        print(f"[NoToTrainer] –ü—Ä–æ–ø—É—Å–∫–∞—î–º–æ model.to({device})")
        return model
# üíæ –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ GPU + CPU offload
model_name = "mistralai/Mistral-7B-v0.1"

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    # device_map="auto",  # —Ü–µ –≤–∞–∂–ª–∏–≤–æ –¥–ª—è offloading
    torch_dtype=torch.bfloat16,
    offload_folder="./offload",
    low_cpu_mem_usage=True
)

# ü™õ –í–º–∏–∫–∞—î–º–æ –ø–æ—Ç—Ä—ñ–±–Ω—ñ —Ä–µ–∂–∏–º–∏
model.gradient_checkpointing_enable()
model.enable_input_require_grads()
model.config.use_cache = False

# üîß LoRA –∫–æ–Ω—Ñ—ñ–≥
peft_config = LoraConfig(
    r=8,
    lora_alpha=16,
    target_modules=["q_proj", "v_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, peft_config)
model = model.to("cuda")
def format_example(example):
    source = example.get("metadata", {}).get("source", "")
    output = example["output"]
    if source and output:
        output += f"\n\n–î–µ—Ç–∞–ª—å–Ω—ñ—à–µ: {source}"

    text = f"### –Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è:\n{example['instruction']}\n\n### –í—Ö—ñ–¥:\n{example['input']}\n\n### –í–∏—Ö—ñ–¥:\n{output}"

    tokenized = tokenizer(
        text,
        truncation=True,
        max_length=512,
        padding="max_length",
        return_tensors="pt"
    )

    return {
        "input_ids": tokenized["input_ids"][0],
        "attention_mask": tokenized["attention_mask"][0],
        "labels": tokenized["input_ids"][0]
    }

tokenized_dataset = dataset.map(format_example)

# ‚öôÔ∏è Training arguments
training_args = TrainingArguments(
    output_dir="./lora_mistral",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    logging_steps=10,
    save_steps=500,
    logging_dir="./logs",
    bf16=True,           # –∞–±–æ fp16=True, —è–∫—â–æ GPU –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î bf16
    optim="adamw_torch"
)

trainer = NoToTrainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
)

# üöÄ Go
trainer.train()

# üíæ Save result
model.save_pretrained("lora_mistral_adapter")
tokenizer.save_pretrained("lora_mistral_adapter")