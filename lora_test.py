import regex,torch, json
import streamlit as st
from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM

base_model_name = "google/gemma-2-2b"
lora_path = "lora_only"  # –ó–º—ñ–Ω—ñ—Ç—å –Ω–∞ —Å–≤—ñ–π —à–ª—è—Ö
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
TEMPERATURE=0.3
TOP_K=5
TOP_P=0.3
REPETITION_PENALTY=1.2
MAX_NEW_TOKENS=256
@st.cache_resource
def load_model():
    try:
        tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        base_model = AutoModelForCausalLM.from_pretrained(
            base_model_name,
            torch_dtype=torch.float16 if device.type == 'cuda' else torch.float32,
            attn_implementation='eager'  # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—ó –¥–ª—è Gemma-2
        )
        model = PeftModel.from_pretrained(base_model, lora_path)
        model = model.merge_and_unload()
        model.to(device)
        model.eval()
        return tokenizer, model
    except Exception as e:
        st.error(f"–ü–æ–º–∏–ª–∫–∞ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–Ω—è –º–æ–¥–µ–ª—ñ: {e}")
        return None, None

# –§–æ—Ä–º–∞—Ç –ø—Ä–æ–º–ø—Ç–∞, —â–æ–± –º–æ–¥–µ–ª—å –≤–∏–≤–æ–¥–∏–ª–∞ –≤–∞–ª—ñ–¥–Ω–∏–π JSON –∑ –ø–æ–ª—è–º–∏ function_call —ñ text
PROMPT_TEMPLATE = """
–Ü–Ω—Å—Ç—Ä—É–∫—Ü—ñ—è: –¢–∏ HR-–∞—Å–∏—Å—Ç–µ–Ω—Ç –¥–ª—è Redmine.
–¢–≤–æ—è –∑–∞–¥–∞—á–∞ - –≤—ñ–¥–ø–æ–≤—ñ–¥–∞—Ç–∏ –Ω–∞ –∑–∞–ø–∏—Ç–∏ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—ñ–≤, –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—á–∏ —Ñ—É–Ω–∫—Ü—ñ—ó API Redmine.

- –Ø–∫—â–æ –∑–∞–ø–∏—Ç –ø–æ—Ç—Ä–µ–±—É—î –≤–∏–∫–æ–Ω–∞–Ω–Ω—è —Ñ—É–Ω–∫—Ü—ñ—ó, –ø–æ–≤–µ—Ä–Ω–∏ JSON –∑ –ø–æ–ª—è–º–∏:
  - "function_calls" ‚Äî —Å–ø–∏—Å–æ–∫ –æ–±'—î–∫—Ç—ñ–≤, –¥–µ –∫–æ–∂–µ–Ω –º–∞—î:
    - "name" ‚Äî –Ω–∞–∑–≤–∞ —Ñ—É–Ω–∫—Ü—ñ—ó,
    - "arguments" ‚Äî —Å–ª–æ–≤–Ω–∏–∫ –∞—Ä–≥—É–º–µ–Ω—Ç—ñ–≤ —Ñ—É–Ω–∫—Ü—ñ—ó.
  - "text" ‚Äî —Ç–µ–∫—Å—Ç–æ–≤–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É.

- –Ø–∫—â–æ —Ñ—É–Ω–∫—Ü—ñ—è –Ω–µ –ø–æ—Ç—Ä—ñ–±–Ω–∞, –ø–æ–≤–µ—Ä–Ω–∏ JSON –∑ –ø–æ—Ä–æ–∂–Ω—ñ–º —Å–ø–∏—Å–∫–æ–º "function_calls" —ñ –≤—ñ–¥–ø–æ–≤—ñ–¥–Ω–∏–º "text".

**–í–ê–ñ–õ–ò–í–û:** –í—ñ–¥–ø–æ–≤—ñ–¥—å –º–∞—î –±—É—Ç–∏ —Å—Ç—Ä–æ–≥–æ –≤–∞–ª—ñ–¥–Ω–∏–º JSON —ñ –º—ñ—Å—Ç–∏—Ç–∏ –≤–∞–ª—ñ–¥–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ "function_calls", "name", "arguments", "text".

–ü—Ä–∏–∫–ª–∞–¥–∏:

–ó–∞–ø–∏—Ç: "–ü–æ–∫–∞–∂–∏ —Å—Ç–∞—Ç—É—Å –∑–∞–≤–¥–∞–Ω–Ω—è #343544"
–í—ñ–¥–ø–æ–≤—ñ–¥—å:
{{
  "function_calls": [
    {{
      "name": "get_issue_status",
      "arguments": {{"issue_id": "343544"}}
    }}
  ],
  "text": "–°—Ç–∞—Ç—É—Å –∑–∞–≤–¥–∞–Ω–Ω—è #343544: "
}}

–ó–∞–ø–∏—Ç: "–ü—Ä–∏–≤—ñ—Ç, —è–∫ —Å–ø—Ä–∞–≤–∏?"
–í—ñ–¥–ø–æ–≤—ñ–¥—å:
{{
  "function_calls": [],
  "text": "–ü—Ä–∏–≤—ñ—Ç! –ß–∏–º –º–æ–∂—É –¥–æ–ø–æ–º–æ–≥—Ç–∏?"
}}

---

–ó–∞–ø–∏—Ç:
{input_text}

–í—ñ–¥–ø–æ–≤—ñ–¥—å —É —Ñ–æ—Ä–º–∞—Ç—ñ JSON:
"""

def build_prompt(input_text: str) -> str:
    return PROMPT_TEMPLATE.format(input_text=input_text)

def generate_response(
    tokenizer, 
    model, 
    prompt: str, 
    max_new_tokens=MAX_NEW_TOKENS, 
    temperature=TEMPERATURE, 
    top_k=TOP_K, 
    top_p=TOP_P, 
    repetition_penalty=REPETITION_PENALTY
    ):
    inputs = tokenizer(prompt, return_tensors="pt").to(device)
    with torch.no_grad():
        output_ids = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=temperature,
            top_k=top_k,
            top_p=top_p,
            repetition_penalty=repetition_penalty,
            pad_token_id=tokenizer.eos_token_id,
            eos_token_id=tokenizer.eos_token_id
        )
    generated_tokens = output_ids[0][inputs["input_ids"].shape[1]:]
    response_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)
    return response_text.strip()
def parse_response(response_text):
    """
    –ü–∞—Ä—Å–∏—Ç—å JSON-–≤—ñ–¥–ø–æ–≤—ñ–¥—å —É –≤–∏–≥–ª—è–¥—ñ —Ä—è–¥–∫–∞.
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–ª–æ–≤–Ω–∏–∫ –∑ –∫–ª—é—á–∞–º–∏ 'function_calls' —Ç–∞ 'text' –∞–±–æ None –ø—Ä–∏ –ø–æ–º–∏–ª—Ü—ñ.
    """
    try:
        # –í–∏–¥–∞–ª—è—î–º–æ –∑–∞–π–≤—ñ –∫–æ–º–∏ –ø–µ—Ä–µ–¥ –∑–∞–∫—Ä–∏–≤–∞—é—á–∏–º–∏ –¥—É–∂–∫–∞–º–∏ (–æ–±‚Äô—î–∫—Ç—ñ–≤ —ñ –º–∞—Å–∏–≤—ñ–≤)
        cleaned_text = response_text
        # –ü—Ä–∏–±–ª–∏–∑–Ω–∏–π —Å–ø–æ—Å—ñ–± –≤–∏–¥–∞–ª–∏—Ç–∏ –∑–∞–π–≤—ñ –∫–æ–º–∏ (–º–æ–∂–Ω–∞ –ø–æ–∫—Ä–∞—â–∏—Ç–∏ –∑–∞ –ø–æ—Ç—Ä–µ–±–∏)
        import re
        cleaned_text = re.sub(r',(\s*[}\]])', r'\1', cleaned_text)

        data = json.loads(cleaned_text)
        # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞ –Ω–∞—è–≤–Ω—ñ—Å—Ç—å –ø–æ—Ç—Ä—ñ–±–Ω–∏—Ö –∫–ª—é—á—ñ–≤
        if "function_calls" in data and "text" in data:
            return {
                "function_calls": data["function_calls"],
                "text": data["text"]
            }
        else:
            print("–£ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –≤—ñ–¥—Å—É—Ç–Ω—ñ –∫–ª—é—á—ñ 'function_calls' –∞–±–æ 'text'")
            return None
    except json.JSONDecodeError as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥—É JSON: {e}")
        return None
def create_mock_api_response(function_call):
    func_name = function_call['name']
    arguments = function_call['arguments']

    mock_responses = {
        'get_issue_by_date': f"–ó–Ω–∞–π–¥–µ–Ω–æ 3 –∑–∞–≤–¥–∞–Ω–Ω—è –Ω–∞ {arguments.get('value_1', '–≤–∫–∞–∑–∞–Ω—É –¥–∞—Ç—É')}",
        'get_issue_by_id': f"–ó–∞–≤–¥–∞–Ω–Ω—è #{arguments.get('value_1', 'ID')}: –†–æ–∑—Ä–æ–±–∫–∞ –Ω–æ–≤–æ—ó —Ñ—É–Ω–∫—Ü—ñ—ó (–°—Ç–∞—Ç—É—Å: –í —Ä–æ–±–æ—Ç—ñ)",
        'get_issue_by_name': f"–ó–∞–≤–¥–∞–Ω–Ω—è '{arguments.get('value_1', '–Ω–∞–∑–≤–∞')}': –ê–∫—Ç–∏–≤–Ω–µ –∑–∞–≤–¥–∞–Ω–Ω—è",
        'get_issue_status': f"–°—Ç–∞—Ç—É—Å –∑–∞–≤–¥–∞–Ω–Ω—è: –í —Ä–æ–±–æ—Ç—ñ",
        'get_issue_hours': f"–í–∏—Ç—Ä–∞—á–µ–Ω–æ –≥–æ–¥–∏–Ω: 12.5",
        'fill_issue_hours': f"–ì–æ–¥–∏–Ω–∏ —É—Å–ø—ñ—à–Ω–æ –∑–∞–ø–æ–≤–Ω–µ–Ω—ñ: {arguments.get('value_2', 'N')} –≥–æ–¥.",
        'get_user_status': "–í–∞—à —Å—Ç–∞—Ç—É—Å: –ù–∞ —Ä–æ–±–æ—Ç—ñ",
        'set_user_status': f"–°—Ç–∞—Ç—É—Å –∑–º—ñ–Ω–µ–Ω–æ –Ω–∞: {arguments.get('value_1', '–Ω–æ–≤–∏–π —Å—Ç–∞—Ç—É—Å')}",
        'create_issue': f"–°—Ç–≤–æ—Ä–µ–Ω–æ –∑–∞–≤–¥–∞–Ω–Ω—è: '{arguments.get('value_1', '–Ω–∞–∑–≤–∞')}'",
        'assign_issue': f"–ó–∞–≤–¥–∞–Ω–Ω—è –ø—Ä–∏–∑–Ω–∞—á–µ–Ω–æ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É: {arguments.get('value_2', '–∫–æ—Ä–∏—Å—Ç—É–≤–∞—á')}",
        'get_wiki_info': f"Wiki —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—è –∑–Ω–∞–π–¥–µ–Ω–∞ –ø—Ä–æ: {arguments.get('value_1', '—Ç–µ–º—É')}"
    }
    return mock_responses.get(func_name, "–§—É–Ω–∫—Ü—ñ—è –≤–∏–∫–æ–Ω–∞–Ω–∞ —É—Å–ø—ñ—à–Ω–æ")

# --- Streamlit UI ---
st.set_page_config(page_title="HR Assistant Demo", page_icon="ü§ñ", layout="wide")

st.title("ü§ñ HR Assistant –¥–ª—è Redmine")

tokenizer, model = load_model()
if tokenizer is None or model is None:
    st.stop()

col1, col2 = st.columns([1, 1])

with col1:
    st.subheader("–í–∞—à –∑–∞–ø–∏—Ç")
    input_text = st.text_area("–í–≤–µ–¥—ñ—Ç—å –∑–∞–ø–∏—Ç —É–∫—Ä–∞—ó–Ω—Å—å–∫–æ—é:", height=120)
    max_tokens = st.slider("–ú–∞–∫—Å. —Ç–æ–∫–µ–Ω—ñ–≤ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ:", 50, 512, MAX_NEW_TOKENS)
    temperature = st.slider("–¢–µ–º–ø–µ—Ä–∞—Ç—É—Ä–∞ –≥–µ–Ω–µ—Ä–∞—Ü—ñ—ó:", 0.1, 1.0, TEMPERATURE, 0.1)
    top_k = st.slider("Top-K", 1, 100, TOP_K)
    top_p = st.slider("Top-P", 0.0, 1.0, TOP_P)
    repetition_penalty = st.slider("Repetition Penalty", 1.0, 2.0, REPETITION_PENALTY)
    generate_btn = st.button("–ó–≥–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—å")

with col2:
    st.subheader("–í—ñ–¥–ø–æ–≤—ñ–¥—å –∞—Å–∏—Å—Ç–µ–Ω—Ç–∞")
    if generate_btn:
        if not input_text.strip():
            st.warning("–í–≤–µ–¥—ñ—Ç—å —Ç–µ–∫—Å—Ç –∑–∞–ø–∏—Ç—É.")
        else:
            with st.spinner("–ì–µ–Ω–µ—Ä—É—î–º–æ –≤—ñ–¥–ø–æ–≤—ñ–¥—å..."):
                prompt = build_prompt(input_text)
                response = generate_response(
                    tokenizer, 
                    model,
                    prompt,
                    max_tokens,
                    temperature=temperature,
                    top_k=top_k,
                    top_p=top_p,
                    repetition_penalty=repetition_penalty,
                )
                parsed = parse_response(response)
                if parsed:
                    func_call = parsed["function_calls"]
                    text = parsed["text"]
                    print(f"Response: {parsed}")

                    st.markdown("### –¢–µ–∫—Å—Ç–æ–≤–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å:")
                    st.write(text)

                    st.markdown("### –í–∏–∫–ª–∏–∫ —Ñ—É–Ω–∫—Ü—ñ—ó:")
                    st.json(func_call)

                    # –ú–æ–∫–æ–≤–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å API
                    mock_api_response = create_mock_api_response(func_call[0])
                    st.success(f"üì° –ú–æ–∫–æ–≤–∞ –≤—ñ–¥–ø–æ–≤—ñ–¥—å API: {mock_api_response}")

                else:
                    st.warning("–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑–ø—ñ–∑–Ω–∞—Ç–∏ JSON —É –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ.")
                st.text_area("–í—ñ–¥–ø–æ–≤—ñ–¥—å –º–æ–¥–µ–ª—ñ:", value=response, height=200)
                